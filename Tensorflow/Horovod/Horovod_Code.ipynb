{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Tensorflow and horovod\n",
    "\n",
    "In the markdown cells, I'll be showing the code changed needed\n",
    "\n",
    "In the end, the horovod loop needs to be in a file, I'll add that file to the repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-09 12:00:16.064807: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Horovod imports - \n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "import horovod\n",
    "import horovod.tensorflow as hvd\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally you should be having a dataset loaded, not downloaded :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "(mnist_images, mnist_labels), _ = \\\n",
    "    tf.keras.datasets.mnist.load_data(path='mnist.npz')\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (tf.cast(mnist_images[..., tf.newaxis] / 255.0, tf.float32),\n",
    "     tf.cast(mnist_labels, tf.int64))\n",
    ")\n",
    "dataset = dataset.repeat().shuffle(10000).batch(128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "mnist_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, [3, 3], activation='relu'),\n",
    "    tf.keras.layers.Conv2D(64, [3, 3], activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "loss = tf.losses.SparseCategoricalCrossentropy()\n",
    "opt = tf.optimizers.Adam(0.001)\n",
    "checkpoint_dir = './checkpoints'\n",
    "checkpoint = tf.train.Checkpoint(model=mnist_model, optimizer=opt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Horovod - Scale the learning rate according to number of workers\n",
    "\n",
    "```python\n",
    "opt = tf.optimizers.Adam(0.001 * hvd.size())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training step\n",
    "\n",
    "@tf.function\n",
    "def training_step(images, labels, first_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        probs = mnist_model(images, training=True)\n",
    "        loss_value = loss(labels, probs)\n",
    "    #  tape = hvd.DistributedGradientTape(tape)\n",
    "    grads = tape.gradient(loss_value, mnist_model.trainable_variables)\n",
    "    opt.apply_gradients(zip(grads, mnist_model.trainable_variables))\n",
    "    # if first_batch:\n",
    "    # :\n",
    "    # :\n",
    "    # :\n",
    "    return loss_value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Horovod - Replace gradientTape with DistributedGradientTape\n",
    "\n",
    "```python\n",
    "tape = hvd.DistributedGradientTape(tape)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Horovod: broadcast initial variable states from rank 0 to all other processes.\n",
    "This is necessary to ensure consistent initialization of all workers when\n",
    "training is started with random weights or restored from a checkpoint.\n",
    "\n",
    "Note: broadcast should be done after the first gradient step to ensure optimizer\n",
    "initialization.\n",
    "```py\n",
    "        if first_batch:\n",
    "            hvd.broadcast_variables(mnist_model.variables, root_rank=0)\n",
    "            hvd.broadcast_variables(opt.variables(), root_rank=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #0\tLoss: 0.240834\n",
      "Step #10\tLoss: 0.127244\n",
      "Step #20\tLoss: 0.179432\n",
      "Step #30\tLoss: 0.222633\n",
      "Step #40\tLoss: 0.245975\n",
      "Step #50\tLoss: 0.230597\n",
      "Step #60\tLoss: 0.215725\n",
      "Step #70\tLoss: 0.139243\n",
      "Step #80\tLoss: 0.161209\n",
      "Step #90\tLoss: 0.174246\n"
     ]
    }
   ],
   "source": [
    "for batch, (images, labels) in enumerate(dataset.take(100)):\n",
    "    loss_value = training_step(images, labels)\n",
    "    if batch % 10 == 0:\n",
    "        print(\"Step #%d\\tLoss: %.6f\" % (batch, loss_value))\n",
    "        #checkpoint.save(checkpoint_dir) So you can save checkpoint :)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Horovod - Scale the dataset based on num workers\n",
    "\n",
    "```python\n",
    "for batch, (images, labels) in enumerate(dataset.take(10000 // hvd.size())):\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Horovod - Checkpoint saving to be only done in rank one\n",
    "\n",
    "```py\n",
    "if hvd.rank() == 0:\n",
    "    checkpoint.save(checkpoint_dir)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All this code will have to be wrapped in a main function, with little more boilerplate code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f401cf1dbab24df559ae8789ef7eacae25a0fecff741eceb08aecb7249ab0875"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
